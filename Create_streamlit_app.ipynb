{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v4ndi/DataStructuresImplementation/blob/master/Create_streamlit_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvlYkCQ9vFiy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07438809-9773-485e-be25-85e895034f59"
      },
      "source": [
        "!pip install -q streamlit"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Инициализация модели и токенайзера\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/rugpt3large_based_on_gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"ai-forever/rugpt3large_based_on_gpt2\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "# Функция генерации текста\n",
        "def generate_text(prompt, max_new_tokens, min_new_tokens):\n",
        "    encoded_input = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    output = model.generate(\n",
        "        **encoded_input,\n",
        "        num_beams=6,\n",
        "        do_sample=True,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        min_new_tokens=min_new_tokens,\n",
        "        temperature=1.5,\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True), len(output[0])\n",
        "\n",
        "\n",
        "# Вычисление перплексии\n",
        "def calculate_perplexity(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "    loss = outputs.loss.item()\n",
        "    perplexity = torch.exp(torch.tensor(loss))\n",
        "    return perplexity.item()\n",
        "\n",
        "# Вычисление вероятностной оценки повторений\n",
        "def calculate_repetition_rate(text):\n",
        "    vectorizer = CountVectorizer()\n",
        "    word_counts = vectorizer.fit_transform([text]).toarray()\n",
        "    unique_words = np.count_nonzero(word_counts)\n",
        "    total_words = sum(word_counts[0])\n",
        "    return 1 - unique_words / total_words\n",
        "\n",
        "\n",
        "# Streamlit интерфейс\n",
        "st.title(\"Генерация текста с оценкой качества\")\n",
        "\n",
        "# Ввод для генерации текста\n",
        "prompt = st.text_area(\"Введите текст запроса\", \"Напиши аннотацию к научной работе на тему...\")\n",
        "max_new_tokens = st.slider(\"Максимальное количество новых токенов\", 50, 500, 150)\n",
        "min_new_tokens = st.slider(\"Минимальное количество новых токенов\", 10, 100, 10)\n",
        "\n",
        "if st.button(\"Сгенерировать текст\"):\n",
        "    generated_text, _ = generate_text(prompt, max_new_tokens, min_new_tokens)\n",
        "    perplexity = calculate_perplexity(generated_text)\n",
        "    repetition_rate = calculate_repetition_rate(generated_text)\n",
        "\n",
        "    # Отображение сгенерированного текста и показателей\n",
        "    st.subheader(\"Сгенерированный текст\")\n",
        "    st.write(generated_text)\n",
        "\n",
        "    st.subheader(\"Оценки качества текста\")\n",
        "    st.write(f\"Перплексия: {perplexity:.2f}\")\n",
        "    st.write(f\"Уровень повторов: {repetition_rate:.2f}\")\n",
        "\n",
        "\n",
        "# Ввод для продолжения текста\n",
        "continuation_prompt = st.text_area(\"Введите текст для продолжения\", \"Концепция качества исходного кода...\")\n",
        "if st.button(\"Сгенерировать продолжение\"):\n",
        "    min_len_paragraph = len(generated_text) * 2 if len(generated_text) * 2 < 330 else 330\n",
        "    generated_paragraph, _ = generate_text(continuation_prompt, 330, min_len_paragraph)\n",
        "    perplexity = calculate_perplexity(generated_paragraph)\n",
        "    repetition_rate = calculate_repetition_rate(generated_paragraph)\n",
        "\n",
        "    # Отображение сгенерированного параграфа и показателей\n",
        "    st.subheader(\"Сгенерированный параграф\")\n",
        "    st.write(generated_paragraph)\n",
        "\n",
        "    st.subheader(\"Оценки качества параграфа\")\n",
        "    st.write(f\"Перплексия: {perplexity:.2f}\")\n",
        "    st.write(f\"Уровень повторов: {repetition_rate:.2f}\")"
      ],
      "metadata": {
        "id": "meJ36PefNftd",
        "outputId": "7cc7d02e-bb00-44da-d9e9-4345dfedbd00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d7c5fb0-ccbf-49d9-a244-1499383b01c7",
        "id": "ZAyqQCQVOoxC"
      },
      "source": [
        "!npm install localtunnel"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\n",
            "added 22 packages, and audited 23 packages in 2s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues, run:\n",
            "  npm audit fix\n",
            "\n",
            "Run `npm audit` for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "Zv912rRAN0fs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "XTGAizLhOIgC",
        "outputId": "44067ebc-3721-4bf9-c7c0-a1970ce4ef64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your url is: https://chilly-rocks-fly.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVz-H__pOoxG"
      },
      "source": [
        "[![ko-fi](https://www.ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/Y8Y3VYYE)"
      ]
    }
  ]
}